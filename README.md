# MCP Agent

![ai](https://github.com/user-attachments/assets/3a872acc-3bee-4762-a22c-a28432923f46)

## Diagram

![diagram](./images/diagram.png)

## Overview

This project implements a LangGraph agent based on the **ReWOO (Reasoning WithOut Observation)** pattern. Similar to the ReAct (Reasoning and Acting) pattern, it reasons and acts, but aims to reduce the amount and complexity of prompts, especially when using Small LLMs (SLMs), by pre-defining the necessary evidence during the planning phase instead of explicitly separating an Observation step.

It also includes several optimization logics, such as tool filtering, plan validation and correction, and evidence management, for efficient interaction with the MCP (Model Context Protocol) server.

## Architecture: LangGraph Nodes

This agent executes the ReWOO workflow using the following LangGraph nodes:

1.  **`tool_filter`**: At the start of agent execution, filters the most relevant tools for the current task based on the user query and descriptions of all available tools.
2.  **`planner`**: Based on the filtered tool information, generates a step-by-step execution plan (in YAML format) to resolve the user query. Also handles plan correction logic upon errors.
3.  **`plan_parser`**: Parses the raw YAML plan generated by the `planner` and validates its structural integrity. May include logic for automatic correction using LLM upon parsing/validation errors.
4.  **`plan_validator`**: Finally validates whether the tool names included in the parsed plan exist in the valid tool list selected during the `tool_filter` step.
5.  **`tool_selector`**: Selects the tool for the current step to be executed from the validated plan.
6.  **`tool_input_preparer`**: Prepares the input for the selected tool. In this process, it replaces placeholders (#E1, etc.) with actual values by referencing the evidence collected in previous steps.
7.  **`tool_executor`**: Executes the actual tool (via the MCP server) with the prepared input and retrieves the result.
8.  **`evidence_processor`**: Processes the tool execution result and stores it in the `evidence` dictionary. (Currently a placeholder, further logic can be implemented).
9.  **`generate_final_answer`**: When all plan steps are completed or the plan finishes without tool calls, synthesizes the final answer by compiling all collected evidence.

## Project Structure

```
.
├── agent/             # ReWOO agent core logic
│   ├── graph.py       # LangGraph graph definition
│   ├── state.py       # Agent state definition (ReWOOState)
│   ├── nodes/         # Graph node implementations (planner, parser, executor, etc.)
│   ├── prompts/       # LLM prompt definitions
│   └── utils.py       # Utility functions
├── core/              # Core utilities
│   └── llm_loader.py  # LLM instance loading and management
├── tests/             # Test code (unit/integration)
├── images/            # Images for README
├── mcp.json           # MCP server configuration file
├── main.py            # Agent execution script
├── .env.example       # Environment variable setup example
└── README.md          # Project documentation
```

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <repository_url>
    cd mcp-agent
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # macOS/Linux
    # venv\Scripts\activate    # Windows
    ```

3.  **Install required libraries:**
    ```bash
    pip install -r requirements.txt # (If requirements.txt exists)
    # Or install necessary libraries directly:
    pip install langchain langgraph langchain-community langchain-openai langchain-google-genai langchain-anthropic langchain_mcp_adapters python-dotenv httpx openai
    # For using local models:
    # pip install llama-cpp-python
    ```

4.  **Configure environment variables:**
    *   Copy the `.env.example` file to create a `.env` file.
    *   Enter the necessary information in the `.env` file according to the LLM provider you will use.

    **Using API-based LLMs:**

    *   You must set **one API key** and **MODEL_NAME**.
        *   **OpenRouter:**
            ```dotenv
            OPEN_ROUTER_API_KEY="sk-or-..."
            MODEL_NAME="qwen/qwen3-14b:free" # Or other OpenRouter model ID
            # LLM_PROVIDER="openrouter" # Explicit specification (optional)
            ```
        *   **Google Gemini:**
            ```dotenv
            GOOGLE_API_KEY="AIz..."
            MODEL_NAME="gemini-1.5-pro-latest" # Or other Gemini model ID
            # LLM_PROVIDER="google" # Explicit specification (optional)
            ```
        *   **Anthropic Claude:**
            ```dotenv
            ANTHROPIC_API_KEY="sk-ant-..."
            MODEL_NAME="claude-3-5-sonnet-20240620" # Or other Claude model ID
            # LLM_PROVIDER="anthropic" # Explicit specification (optional)
            ```
    *   **Important:** When using an API key, you must set the `MODEL_NAME` environment variable. If multiple API keys are set simultaneously, you must explicitly set `LLM_PROVIDER` to specify the service to use.

    **Using local LlamaCpp models:**

    *   Do not set any API keys.
    *   Set the path to the model file.
        ```dotenv
        MODEL_PATH="/path/to/your/model.gguf"
        # LLM_PROVIDER="local" # Explicit specification (optional)
        # N_CTX=8192         # Optional LlamaCpp parameter
        # N_GPU_LAYERS=-1    # Optional LlamaCpp parameter
        # N_BATCH=512        # Optional LlamaCpp parameter
        ```

    **Other environment variables (Optional):**

    *   `TEMPERATURE`: LLM temperature (default: 0.2)
    *   `MAX_TOKENS`: Maximum generation tokens (default: 8192)
    *   `TOP_P`: Top-P sampling (default: 0.95)
    *   `TOP_K`: Top-K sampling (used by Gemini, Claude, LlamaCpp, default: 40)

5.  **Configure MCP Server:**
    *   Set the information for the tool servers the agent will use in the `mcp.json` file. Specify the name, transport method (`transport`), command (`command`), arguments (`args`) or URL (`url`) for each server.
      *   Example:
          ```json
          {
            "servers": {
              "sequential-thinking": {
                "command": "npx",
                "args": [
                  "-y",
                  "@modelcontextprotocol/server-sequential-thinking"
                ]
              }
            }
          }
          ```

## Usage

```bash
python main.py
```
